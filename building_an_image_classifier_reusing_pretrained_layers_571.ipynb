{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Welcome to the project on Working with Custom Loss Function. This project aims to provide an understanding of how we could use the custom defined loss functions along with TensorFlow 2.\n",
    "\n",
    "Though TensorFlow 2 already provides us with a variety of loss functions, knowing how to use a user-defined loss function would be crucial for a machine learning aspirant because often times in real-world industries, it is expected to experiment with various custom defined functions. This exercise is designed to achieve that goal.\n",
    "\n",
    "Skills you will develop:\n",
    "\n",
    "TensorFlow 2\n",
    "Defining Custom Loss Function\n",
    "Python Programming\n",
    "scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Project\n",
    "Objective\n",
    "\n",
    "Fashion-MNIST is a dataset of Zalando's article images —consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label.\n",
    "\n",
    "The objective of the project is - to use Fashion-MNIST data set to identify (predict) different fashion products(articles) from the given images using Machine Learning.\n",
    "\n",
    "We will be following the below steps to solve this problem:\n",
    "\n",
    "Importing the libraries\n",
    "\n",
    "* Loading the data\n",
    "\n",
    "* Splitting the data\n",
    "\n",
    "* Visualizing the Data\n",
    "\n",
    "* Building the Model\n",
    "\n",
    "* Fitting the Model\n",
    "\n",
    "* Evaluating the Model Performance\n",
    "\n",
    "\n",
    "Acknowledgements\n",
    "\n",
    "Cloudxlab is using this “Fashion MNIST” problem for its machine learning learners for learning and practicing. Fashion-MNIST dataset is a collection of fashion article's images provided by Zalando . We thank Zalando Research for hosting the dataset.\n",
    "\n",
    "What are we going to do?\n",
    "\n",
    "We will train a neural network (say model A) on data related to 6 of the classes, and we will train another neural network (say model B) on the remaining 2 classes. Then, we would use the pre-trained weights of model A and tune the last layer so as to classify these 2 classes(this technique is called Transfer Learning), and compare the results of classification obtained using normal training and transfer learning. In this project, we would practically appreciate the use of Transfer Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Dataset\n",
    "Let us load the dataset and trim it to form a shorter dataset, as training a bigger dataset would take a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 1s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trimming the data since it takes lot of time\n",
    "X_train_full = X_train_full[:30000]\n",
    "y_train_full = y_train_full[:30000]\n",
    "\n",
    "X_test = X_test[:5000]\n",
    "y_test = y_test[:5000]\n",
    "\n",
    "# scaling the dataset\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# dividing the dataset into traingin and validation set\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the data sets\n",
    "Let's split the fashion MNIST training set in two:\n",
    "\n",
    "**X_train_A:** all images of all items except for sandals and shirts (classes 5 and 6).\n",
    "**X_train_B:** a much smaller training set of just the first 200 images of sandals or shirts. The validation set and the test set are also split this way, but without restricting the number of images.\n",
    "\n",
    "Why are we doing this?\n",
    "\n",
    "We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (sneakers, ankle boots, coats, t-shirts, etc.) are somewhat similar to classes in set B (sandals and shirts). However, since we are using Dense layers, only patterns that occur at the same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can be detected anywhere on the image, as we will see in the CNN chapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A), (X[y_5_or_6], y_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_dataset on the X_train and y_train.\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the split_dataset on the X_valid and y_valid.\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the split_dataset on the X_test and y_test.\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Fit the Model A\n",
    "Let us define the model for the classification of data set A that we have created previously.\n",
    "\n",
    "Later the trained weights of this model will be used for the classification task of data B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a keras neural network as follows:\n",
    "\n",
    "* Add keras.layers.Flatten to flatten the input image to the model.\n",
    "\n",
    "* Add 5 dense layers with n_hidden number of neurons and selu activation function.\n",
    "\n",
    "* Add a final dense layer with 8 neurons and softmax activation function(for classifying 8 classes of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the compiling part of the model:\n",
    "\n",
    "* Set \"sparse_categorical_crossentropy\" as loss.\n",
    "\n",
    "* Set keras.optimizers.SGD(lr=1e-3) as optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss= \"sparse_categorical_crossentropy\",\n",
    "    optimizer= keras.optimizers.SGD(lr=1e-3),\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train model_A on X_train_A and y_train_A for 5 epochs , and validation_data=(X_valid_A, y_valid_A) using model_A.fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19875 samples, validate on 4014 samples\n",
      "Epoch 1/5\n",
      "19875/19875 [==============================] - 3s 137us/sample - loss: 0.8034 - accuracy: 0.7492 - val_loss: 0.4698 - val_accuracy: 0.8508\n",
      "Epoch 2/5\n",
      "19875/19875 [==============================] - 2s 109us/sample - loss: 0.4269 - accuracy: 0.8593 - val_loss: 0.4009 - val_accuracy: 0.8642\n",
      "Epoch 3/5\n",
      "19875/19875 [==============================] - 2s 93us/sample - loss: 0.3715 - accuracy: 0.8725 - val_loss: 0.3487 - val_accuracy: 0.8802\n",
      "Epoch 4/5\n",
      "19875/19875 [==============================] - 2s 93us/sample - loss: 0.3419 - accuracy: 0.8812 - val_loss: 0.3296 - val_accuracy: 0.8876\n",
      "Epoch 5/5\n",
      "19875/19875 [==============================] - 2s 93us/sample - loss: 0.3246 - accuracy: 0.8855 - val_loss: 0.3258 - val_accuracy: 0.8849\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=5,\n",
    "            validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Fit the Model B\n",
    "Let us define the model for the classification of data set B that we have created previously.\n",
    "\n",
    "Later, let us also examine the classification of B set by using the trained weights of model A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a keras neural network as follows:\n",
    "\n",
    "* Add keras.layers.Flatten to flatten the input image to the model.\n",
    "\n",
    "* Add 5 dense layers with n_hidden number of neurons and selu activation function.\n",
    "\n",
    "* Add a final dense layer with 1 neuron and softmax activation function(for classifying 2 classes of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the compiling part of the model:\n",
    "\n",
    "* Set \"binary_crossentropy\" as loss, as this is binary classification among sandals and shirts.\n",
    "\n",
    "* Set keras.optimizers.SGD(lr=1e-3) as optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B.compile(loss= \"binary_crossentropy\",\n",
    "    optimizer= keras.optimizers.SGD(lr=1e-3),\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5125 samples, validate on 986 samples\n",
      "Epoch 1/5\n",
      "5125/5125 [==============================] - 1s 192us/sample - loss: 7.5487 - accuracy: 0.5050 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "5125/5125 [==============================] - 0s 98us/sample - loss: 7.5487 - accuracy: 0.5050 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "5125/5125 [==============================] - 1s 103us/sample - loss: 7.5487 - accuracy: 0.5050 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "5125/5125 [==============================] - 1s 99us/sample - loss: 7.5487 - accuracy: 0.5050 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "5125/5125 [==============================] - 1s 102us/sample - loss: 7.5487 - accuracy: 0.5050 - val_loss: 7.6246 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "history = model_B.fit(X_train_B, y_train_B, epochs=5,\n",
    "            validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new model based on existing model A\n",
    "Let us first see how many trainable parameters are there for model_B we trained previously.\n",
    "\n",
    "Then we shall create a new model model_B_on_A which has the pre-trained parameters of model_A but customized final dense layer with only 1 neuron.\n",
    "\n",
    "Finally, we shall compare the performance of both the models - model_B and model_B_on_A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 275,801\n",
      "Trainable params: 275,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_B.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are 275,801 trainable parameters for model_B.\n",
    "\n",
    "Now, before creating model_B_on_A(a model based on pre-trained layers of model_A), we shall clone the model_A and set its trained weights so that when you train model_B_on_A, it will not affect model_A.\n",
    "\n",
    "We could copy the model_A architechture using keras.models.clone_model.\n",
    "\n",
    "Create model_A_clone which is the copy of model_A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the weights of model_A using get_weights(), and set the model parameters for model_A_clone using set_weights()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a new model model_B_on_A, based on existing layers of model_A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A= keras.models.Sequential(model_A.layers[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the final dense layer with 1 neuron to the model_B_on_A. Set the activation to \"sigmoid\", as this is a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set all the layers, except the last layer, of model_B_on_A to be non-trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the number of trainable parameters of model_B_on_A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 275,801\n",
      "Trainable params: 51\n",
      "Non-trainable params: 275,750\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_B_on_A.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe there are only 51 parameters to train in model_B_on_A, while there are as many as 275,801 trainable parameters for model_B.\n",
    "\n",
    "Compile the model model_B_on_A by using model.compile.\n",
    "\n",
    "Set loss=\"binary_crossentropy\".\n",
    "\n",
    "Set optimizer=keras.optimizers.SGD(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "         optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "         metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5125 samples, validate on 986 samples\n",
      "Epoch 1/5\n",
      "5125/5125 [==============================] - 1s 162us/sample - loss: 0.3706 - accuracy: 0.8353 - val_loss: 0.2759 - val_accuracy: 0.9128\n",
      "Epoch 2/5\n",
      "5125/5125 [==============================] - 0s 74us/sample - loss: 0.2165 - accuracy: 0.9419 - val_loss: 0.1872 - val_accuracy: 0.9554\n",
      "Epoch 3/5\n",
      "5125/5125 [==============================] - 0s 74us/sample - loss: 0.1553 - accuracy: 0.9707 - val_loss: 0.1453 - val_accuracy: 0.9716\n",
      "Epoch 4/5\n",
      "5125/5125 [==============================] - 0s 73us/sample - loss: 0.1234 - accuracy: 0.9811 - val_loss: 0.1213 - val_accuracy: 0.9797\n",
      "Epoch 5/5\n",
      "5125/5125 [==============================] - 0s 75us/sample - loss: 0.1039 - accuracy: 0.9852 - val_loss: 0.1059 - val_accuracy: 0.9848\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=5,\n",
    "                   validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the models\n",
    "Now that we have the two models model_B and model_B_on_A for classifying the B dataset, let us evaluate the performance of the model based on their accuracies on the test data of B data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use evaluate() method on model_B and pass X_test_B and y_test_B as arguments to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "967/967 [==============================] - 0s 60us/sample - loss: 7.6483 - accuracy: 0.4984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.64827382502906, 0.49844882]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use evaluate() method on model_B_on_A and pass X_test_B and y_test_B as arguments to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "967/967 [==============================] - 0s 59us/sample - loss: 0.0992 - accuracy: 0.9886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09924376358383075, 0.98862463]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the accuracies of both models are almost the same.\n",
    "\n",
    "We also see that the performance of model_B_on_A - with as less as 51 trainable parameter - stands to be as great as that of model_Bwith as many as 275,801.\n",
    "\n",
    "So, with very little training, model_B_on_A is performing really well. This saves time and resources even in real-time scenarios. This is the beauty of using pre-trained layers. This method is also known as transfer learning - transferring the knowledge obtained from solving one problem to solving another similar problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
